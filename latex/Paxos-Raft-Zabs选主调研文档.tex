\documentclass{article}
\usepackage{multicol}
\usepackage[UTF8]{ctex}
\usepackage{xcolor}
\usepackage{abstract}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{graphicx}
\usetikzlibrary{graphs}
\usetikzlibrary{arrows, patterns, positioning}

\title{Paxos-Raft-Zab算法调研文档}
\author{金世钰，\textbf{四川蜀天梦图数据科技有限公司}}

% 双栏设置
\setlength{\columnsep}{1cm}

% 超链接设置
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=red,
}

\begin{document}
\maketitle

\begin{abstract}
本文主要介绍了Paxos、Raft和Zab三种目前业界主流采用的共识算法，为分布式图计算主节点选举提供备选算法方案。
\end{abstract}
%\begin{multicols}{2}

\section{前言}
目前，图计算小组开发的分布式图计算模型基于原有Tinkerpop所采用的BSP计算模型，已经能够成功使用多个节点，共同执行Pagerank算法并得出正确结果。其计算流程大致为一个主节点协调多个从节点执行计算任务，最后由主节点综合计算结果。

%\begin{figure}[hb]
%	\centering
%	\includegraphics[scale=0.1]{tp-dis.png}
%	\caption{分布式图计算总体架构}
%	\label{fig:label}
%\end{figure}

该模型涉及到一个协调各节点的Leader节点，但是在该模型中，Leader节点存在单点问题，当Leader节点下线时（节点故障、重启或者被网络隔离开），集群便陷入“群龙无首”的困境，此时分布式计算任务无法派发，整个计算流程就卡死在第一步。所以需要一个算法，使得集群中Leader节点下线时，能够自动选举出新的Leader节点，从而解决Leader节点的单点问题。

本次调研主要是为分布式图计算主节点选举提供参考方案，调研了当前分布式系统用到的主流算法，包括Paxos、Raft和Zab三种算法。这三种算法都是分布式共识算法，本身内容不仅仅局限于集群中主节点选举这一部分，但是根据我们分布式图计算系统的需要，我们暂时只关注于算法中的主节点选举部分，或者只将算法用于主节点选举。 

\section{算法介绍}

\subsection{Paxos}
Paxos算法由2013年图灵奖获得者\emph{Leslie Lamport}在1990率先提出，该算法主要是用于解决分布式系统的一致性问题，也就是分布式中所有成员对某个提议如何达成共识的问题。

Paxos算法在1990年时并没有得到发表，起因是该算法以一种类似于拜占庭将军的形式对算法进行描述，当时的编辑拒绝对此文进行发表，直到8年之后的1998年，有一个团队需要在分布式系统中达成共识，此时Lamport将算法给了他们并且得到了实施，尔后Lamport再次发表了该算法，也就是著名的《The Part-Time Parliament》，但是对读者来说，该故事性的算法描述却仍然难以理解，直到2001年，Lamport再次修改该算法的表述，发表了论文《Paxos Made Simple》，清晰的解释了该算法。随后，Google公司的Mike Burrows在2006年发表了论文《The Chubby lock service for loosely-coupled distributed systems》，其中就使用了Paxos算法作为其一致性算法，此后Paxos算法受到热烈欢迎。

本文主要参考了论文\href{https://lamport.azurewebsites.net/pubs/paxos-simple.pdf}{《Paxos Made Simple》}

Paxos作为分布式系统中的共识性算法，其主要解决的便是分布式系统中的成员就某事如何达成共识，在分布式图计算中，这个共识就是：谁是主节点？

接下来，主要介绍Paxos算法如何达成一致。

Paxos算法有两个确保共识的要求（论文中表述为三个，但是其第二和第三点其实是同一点），分别是
	\begin{enumerate}
		\item 被提议的值可以有多个，但是最终只会有一个被选择
		\item 只有一个值会被最终选择，并且被选中的值只有被真正选中之后才会被告知给其他所有的成员
	\end{enumerate}

在整个算法中，所有成员可以被归类为三种角色，分别是
	\begin{itemize}
		\item proposer，负责提议某个值
		\item acceptor，负责检验proposers提出的值，可以选定值，也可以拒绝值
		\item learner，负责获取最后选定的值
	\end{itemize}

成员之间可以通过互发消息进行通信，整个消息传递使用异步的消息，并且是非拜占庭模型，也就是说
	\begin{itemize}
		\item 成员可以停机、重启，成员需要能够保存住某些信息；
		\item 消息传递需要经历一定的时间段，消息可以重复，可以丢失，但是消息一定是完整的。
	\end{itemize}

	在整个系统中，有以下几点要求

	\begin{enumerate}
		\item 提议的值的形式为<n, v>分别代表编号为n，取值为v的一次提议
		\item accptors必须接受第一个被提议的值，每个acceptor可以接收多个提议的值
		\item 如果一个提议<$n_{1}$, $v_{1}$>被最后选中，那么所有编号大于$n_{1}$的提议如果被选中，那么该提议的值也必须是$v_{1}$。本要求暗含这以下两点
			\begin{enumerate}
				\item 如果一个<n, v>的提议被最终选中，那么每个被acceptor接收的且编号大于n的提议必须拥有值v
				\item 如果一个<n, v>的提议被最终选中，那么每个被proposer发布的且编号大于n的提议的值必须是v
			\end{enumerate}
		\item 系统中的请求分为两类，分别是prepare请求和accept请求两种，都由proposer提出，前者用于发出自己提议的值，后者用于当自己提议的值被大多数acceptor接收时再次请求接收自己的值（因为可能有多个proposer的prepare请求被通过，accept相当于二次确认）
		\item 一个acceptor当接收到prepare请求时，如果没有接收到大于该请求序号的其他请求，那么就接受该prepare请求，并响应其之前接收到的最大的prepare请求的值，并且承诺，不接受其他请求序号小于当前接受请求序号的prepare请求
	\end{enumerate}

整个算法大致可以用两个阶段来进行描述，分别是

	\textbf{第一阶段}

	一个proposer选择一个编号n并且向大多数的acceptor节点发送prepare请求
	如果一个acceptor如果接收到的prepare请求的编号n大于之前接收到的所有prepare请求的编号，那么返回之前接收到的编号最大的prepare请求（可能不存在这样的请求）里的值给proposer，并且承诺，不再响应编号小于n的prepare请求。

	\textbf{第二阶段}
        \begin{enumerate}
	\item 如果一个接收到自己之前发送的编号为n的prepare请求的响应并且这些响应来自大多数的acceptor，那么就向大多数的acceptor发送编号为n，值为自己接收到的响应中的最大的值的accept请求。
        \item 如果一个acceptor接收到了一个编号为n的accept请求，那么就需要接受该请求的值，除非自己又接收到了编号大于n的prepare请求。
        \end{enumerate}


\textbf{常见例子}

在网上很多关于Paxos算法的文章中，都引用了下面的例子。

\begin{figure}[hb]
	\centering
	\includegraphics[scale=0.5]{1.png}
	\caption{proposer A和B都分别向三个Acceptor发送了prepare请求}
	\label{fig:label}
\end{figure}

在图1中，proposer A和B分别向Acceptor X、Y和Z发起了自己的提议，其中，proposer A的prepare请求为[n=2, v=8]，proposer B的prepare请求为[n=4, v=5]，而且Acceptor X、Y先接收到Proposer A和B的prepare请求，Acceptor Z则是先接收到proposer B的请求。出于算法的规则，Acceptor最先接收到的这三个请求是必须被接受的。


\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{2.png}
	\caption{Acceptor对首次接收到的请求的回应}
	\label{fig:label}
\end{figure}
图2显示了acceptor对接收到的最先到达的prepare request的响应过程，Acceptor X和Y都先接收到了来自Proposer A的[n=2, v=8]的prepare request，在这之前它们没有接收到任何prepare request，所以必须接收本次接收到的请求，并向proposer回复一个prepare response，回复内容为[no previous]，代表着之前没有接收到prepare request。并且承诺，不再响应序号小于2的prepare request请求。Acceptor C同理，回复[no previous]的prepare response，同时承诺不响应序号小于4的prepare request。

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{3.png}
	\caption{Acceptor对非首次prepare请求进行回应}
	\label{fig:label}
\end{figure}

图3显示了acceptor对后续到达的prepare request的响应过程。对Acceptor X而言，对后续到达的来自proposer B的[n=4, v=5]的请求，由于序号大于之前接受的序号为2的来自proposer A的prepare request中的序号，所以接收本次prepare request，并且将之前接受的prepare request内容作为prepare response返回给proposer B，并且承诺以后不响应序号小于4的请求，Acceptor Y同理，但是对于Acceptor C而言，后续到达的来自proposer A的[n=2, v=8]的请求由于序号2小于之前接收到的来自proposer B的[n=4, v=5]的prepare request中的序号，所以对此请求不予理会。

注意，在这个例子中，此时所有的acceptor都做出了一个承诺，不接受序号小于4的请求。

至此，所有Acceptor对所有Proposer提出的prepare request请求处理完毕，Proposer A总共收到了2次prepare response响应，B同理，都得到了3个Acceptor中的大部分响应（同意），所以Proposer A和B都将会发起下一次的accept request请求，再次确认自己的值被接受。注意，在发起accept request时，其请求序号和之前的prepare request序号一致，但是对应的值却是选自己之前接收到的prepare response中的值中的最大者，如果prepare response是[no previous]，那么在accept request中的值就是prepare request中的值。


\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{4.png}
	\caption{Proposer B发起accept request}
	\label{fig:label}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{5.png}
	\caption{Learner获取最终接受的提议值}
	\label{fig:label}
\end{figure}

图4中只画出了Proposer B发起的accept request请求，并没有画出Proposer A的accept请求，这不是说Proposer A没有发起，而是它虽然发起了，但是被所有的Acceptor抛弃了，这是因为，在之前所有acceptor处理prepare request请求的时候，都承诺了不响应请求序号小于4的请求，所以Proposer的请求都被抛弃了。另外，Proposer的accept请求是[n=4, v=8]，而不是原来自己发起prepare request的[n=4, v=5]，这是因为在发送accept request时，取值是自己收到的prepare response中的最大值，也就是prepare request[n=2, v=8]中的8。

当acceptor接收到的accept request请求的序号大于等于自己承诺接受的最小序号，在本例中为4，那么就告知所有的learner，自己接受了某个值，在本例中是8，当所有learner发现了大多数的acceptor都接受了某个值，那么该值就是最终的达成共识的值，也就是8，如图5所示。

注意的是，learner可以有多个，在本例子涉及的图中，只画出了一个learner。


\subsection{Raft}

Raft一般指由斯坦福大学的Diego Ongaro和 John Ousterhout在2014年发表的博士论文《In Search of an Understandable Consensus Algorithm》中提出的分布式系统共识算法。

该论文主要提出了一种易于理解的分布式共识算法，其侧重点是可理解性。

本节内容主要来自\href{https://raft.github.io/raft.pdf}{《In Search of an Understandable Consensus Algorithm》}中的第5节。

在Raft的主节点选举中，主要包含两个时间度量，分别是心跳时间和选举时间。另外每个节点至少有3个状态，分别是Follower（从节点），Candidate（主节点候选人），Leader（主节点）。

对于任意Follower节点而言，如果没有在心跳时间内接收到来自Leader的心跳消息，那么就认为此时Leader节点已经下线（可能是Leader节点已关闭或者被网络隔离开），此时将等待一个150ms～300ms的随机选举时间，将自己的状态提升到Candidate，当状态提升完成后，就向集群中所有的节点发起投票请求，请求自己成为新的Leader，如果该请求得到了来自集群大多数（一半以上）的同意，那么自己就成为集群新的Leader，并间隔一定时间向所有的其他节点发送心跳消息，以维持自己的Leader身份。


\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{r1.png}
	\caption{Node B最先进入Candidate状态}
	\label{fig:label}
\end{figure}

图6中有三个Raft节点，其中Node B拥有最小的选举时间，这意味着它最快进入Candidate状态，然后Node B向Node A和Node C发送选举请求，由于Node B和Node C都没有对其他主节点候选人投票，所以都将票投给Node B，同时Node B也给自己投票，因此Node B最后拥有3票投票，也就是得到了集群大多数节点（一半及以上）的同意，因此Node B成为主节点，之后它将定时向其他所有顶点发送心跳信息，以维持自己的主节点身份。

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{state-machine.png}
	\caption{Raft节点状态机}
	\label{fig:label}
\end{figure}

整体来看，Raft的选主算法相当简单明了，不用像Paxos那样要发多次的请求，但是Raft算法仍然有许多细节需要注意。

\textbf{每个节点拥有哪些状态？状态是如何迁移的？}

节点状态可以见图7中的节点状态机，主要包含Follower、Candidate和Leader三种状态，并且每个状态之间可以在一定条件下相互转换，所以节点在刚开始时都处于Follower状态。

\textbf{每个Candidate的投票请求需要包含哪些凭据？}

每个投票请求，需要给出自己竞选Leader的任期term，以及自己作为候选人的标识符CandidateId，除此之外，还需要给出上一次执行的日志标记和日志所在的Leader任期。

\textbf{Follower节点如何判断一个候选人的投票请求是否值得投票？}

如果投票请求中的任期小于自己的当前所拥有的任期（对于Follower节点也有任期），那么直接拒绝，如果投票请求中的日志比当前自己所有的日志标记要小，那么也拒绝。只有请求中的任期比自己当前所拥有的任期要大而且日志标记和自己一致或者更新的，才进行投票。

\textbf{对所有节点而言，是否有需要持久化的值，比如遭遇宕机的情况？}

有。所有节点，必须持久化保存当前任期值，自己所投票支持的Leader节点的标识，以及日志记录。

\textbf{一个Candidate节点的选举过程是怎样的？}

当在心跳超时时间内没有接收到Leader节点的消息时，就进入Candidate状态，首先增加自己的任期，然后投票给自己，重置超时计时器，并向其他所有节点发送投票请求，当投票请求得到集群大多数节点的赞同时，就升任为Leader，如果在还没有升任Leader的过程中收到了Leader的心跳信息，那么将自己从Candidate状态降级为Follower状态，如果选举超时仍然没有得到大多数的投票赞同也没有得到Leader的心跳消息，那么再次发起选举。

Raft作为一个分布式共识算法，就和其论文题目所述一样，是一个易于理解的共识算法，本节仅仅是论文中主节点选举的部分理解，其内容并不止于此，如果需要更好的理解Raft的所有内容，可以参考\href{https://raft.github.io/raft.pdf}{《In Search of an Understandable Consensus Algorithm》}


\subsection{Zab}

Zab是ZooKeeper Atomic Broadcast Protocol的缩写，用于维持ZooKeeper集群中的状态（集群数据副本）一致的协议。整体上来说，Zab和Raft有些许相似之处，尤其在主节点数据操作方面，比如集群里的写操作，都需要先经由主节点处理，再交与其他从节点进行处理，发送到从节点上的请求，从节点要对此请求进行转发，交由主节点处理，所以Zab中的主节点，依然是一个强主节点。Zab中的节点，主要分为三个角色，分别是Leader、Follower以及Observer，其中主节点负责维护整个集群，确保自己的身份声明，定期和Follower和Observer通过心跳信息交流，Follower负责响应Leader的数据写入更新等操作，以及在选举主节点是进行投票，响应客户端的读请求，并将客户端的写请求转交给主节点，Observer就是一个没有主节点选举投票权的特殊Follower，该角色的设立应该是为了增加整个集群的可读性，提高读的命中率。



%\end{multicols}

\end{document}
